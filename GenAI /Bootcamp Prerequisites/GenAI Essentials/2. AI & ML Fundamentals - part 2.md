# 1. Bert Lab
Use hugging face website to get code to test a BERT model on google colllab

# 2. BERT with Lola
BERT is developed in 2018 by google
BERT is model with Encoder only model where model input is Natual Language (NL) and output is encoding(Mathematical representation of NL)
Whereas GPT is decoder only model where input is Encoding and output is NL
Bert is 342 million parameters model also has many variants starting from 100M parameters
GPT is 1.2 trillion 
emergent taks model: are model which were better at unexpected scenarios

BERT is pre-trained on following tasks:
- Masked language model (MLM)
  - Provided input where tokens are masked
  - Think of asking BERT to fill in the blanks for sentenses
- Next sentence prediction (NSP)
  - BERT is provided two sentences A anf B
  - BERT has to predict if B would follow A
- BERT can be fine-tuned to then perform the following tasks
  - Named Entity Recognition (NER)
  - Question Answering (QA): Short answers
  - Sentence Pair Tasks
  - Summerization
  - Feature Extraction / Embeddings
  - Classification
  - and more...
- BERT variants
  - roBERTa
  - DistilBERT
  - ALBERT
  - & so on
 
 
- Chinchilla papar: bigger the model need more data and it works best. it gives relation between model size parameter and performance. paramaeters to data is 1:20 ration.
- Distallation: take essence out 


# 4. Sentence Transformaers
Sentence transformer (aka SBERT) is built on top of BERT. It create a signle vector for an entire sentence. When comparing similar senrence this much more
performance than simply using BERT which looks at every single word.
```
# Example model on hugging face
from sentence_transformers import SentenceTransformer, util
model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
```

# 5. Sentence Transformer Lab
www.sbert.net


```
#Embedding model example
from sentence_transformers import SentenceTransformer

# 1. Load a pretrained Sentence Transformer model
model = SentenceTransformer("all-MiniLM-L6-v2")

# The sentences to encode
sentences = [
    "The weather is lovely today.",
    "It's so sunny outside!",
    "He drove to the stadium.",
]

# 2. Calculate embeddings by calling model.encode()
embeddings = model.encode(sentences)
print(embeddings.shape)
# [3, 384]

# 3. Calculate the embedding similarities
similarities = model.similarity(embeddings, embeddings)
print(similarities)
# tensor([[1.0000, 0.6660, 0.1046],
#         [0.6660, 1.0000, 0.1411],
#         [0.1046, 0.1411, 1.0000]])

```

# 6. Intro to Perceptron
Perceptron is an algorithm for supervised learning of binary classifiers invented in 1943 and the machine built in 1957!
A perceptron is a single-neuron model that was a precursor to larger neural networks.

# 7. Basic Perceptron Network
A basic perceptron has an input layer and output layer. Each layers contains a number of nodes. 
Node between layers have established connection that are weighted.

The amount of nodes in the input layer is dertermined by:
- the number of ddimensions for the inputted vectors
- The input data is a collection of vectors
  - eg a vector that has an X and Y would hav 2 input nodes.
- the input layer is just connections pointsm it does not modify the data

The amount of nodes in the output layer is dertermined by:
- The application of neural net
  - Yes/No classification would only have one output node
    -   it would not matter if there were 1000s of input nodes
- The output nodes (and other layers) can modify (compute) new values based on inputted data

Data moving between nodes are multiple by weights.

The weights will be modified during the training process to produce a bettre outcome.

# 8. Activation Functions
When data arrives to a node that can perform a computation 
- all arriving inputted data is summed
- And then an activation function is triggered

The activation function acts a **gate between nodes** and determines whether output will proceed tothe next layer, The activaton fucntion will determine if a node is active or inactive based on its own output which could be a range between
- 0 to 1
- -1 to 1
There are different kinds of activation functions
- Linear activation functions- can't back propagate
- Non-linear activation function - can back propagate, can stack (have many) layers

# 9. Linear activation function
also know as **identity function**.
- Model is not really learning
- Does not improve upon the error term
- cannot stack layers or back propagates


# 10. Binary Steps
- A binary step function either return 0 or 1 :
  - if th value is 0.0 or less it eill be 0
  - if the value is greater than 0.0 it will be 1
- Can only handle binary classification
  - on/off, True/ False, 0/1
- On of the earliest used activation functions
- Not used much today


# 11. Sigmoid activation Functions
A sigmoid is a Logistic curve that resembles an S-shape
- can handle binary and multi-classification
- can stack layers!
- Range 0 to 1
- Tends to bring the activations to either side of the curve
  - Clear distinctions on prediction
- **One of the most wildely used functions**
- Near the end of the function Y respponds less to X
  - causes a vanishing gradients
  - Network refuses to learn further or is drastically slow
- Sigmoid is analig meaning almost all neurons will fire (be active):
  - activation will be both dense, slower and costly


# 12. Tahn Ativation
A Tahn is the same as a sigmoid function but its scaled (made larger)
- can handle binary and multi-classification
- Can stack layers
- range 1 to -1
- The gradient is stronger (steeper curving)
- Still has vanishing gradient problem like sigmoid
- Tahn vs Sigmoid
  - Tahn can assist in avoid bias in gradients
  - Tahn can outperfomr Sigmoid


# 13. ReLu

Rectified Liner Unit activation function Where the
-  Positive axis is linear
-  The negative axis always returns 0

- Range (0 to infinite) positive axis is unbound (infinite)
- Sigmoid and Tahn for almost all their neurons; this leads to thing to dense, slow and costly
  - RelU will sparsely trigger activation functions because of its negative axis gradient begin zero
  - Less costly and more efficient (speedy)
- The negative-axis with a zero gradient has a side effect called ReLu dying gradient:
  - The gradients will go towards 0 and will be stuck in 0 because variation adjusting due to input or error will have nothing to adjust to. Noder essentially "die"

# 14. Leaky ReLu 
Leaky Rectified Linear Unit activation function here is the:
- The positive axis is linear
- The negative axis has a gentle gradient close to zero
- It is similar to ReLu but is reduces the efect of the ReLu dying gradient.
  - Its "leaky" because the negative axis leaks
  - Causes some nodes not to die
- Prameterized ReLu is leaky ReLu wherre the negative slop is 0.01x
- ReLu6 is ReLu where the positive axis has an upper limit so its not infinite (bound to a max value)

# 15. ELU
Exponential Linear Unit Activaiton:
  - has a slope towards -1 in the negative axiz
  - Has a linear gradient in the positive axis
- Something between a ReLu and Leaky ReLu
- ELU sloping towards -1 causes negative values. it pushes the mean of the activations close to zero.
  - mean activatios close to zero causes:
    - faster learning
    - Convergenve
  - ELU avoids the dying ReLu problem
  - Saturates for large negative numbers

# 16. Swish Activation
- Swish activation: 
  - has a slope that dips and eases out to 0 in the negative axis
  - has a linear gradient in the positive axis
- It was proposed by Google Brain Team as a replacement for ReLu
- Its called swish because of the swishing dip
- It looks similar an ReLu but it is a smooth function
  - it never abruptly changes direction
  - It is non-monotonic (does not remain stable)
- Simlar to ReLu will have sparsity
- Very negative values will zero out
- There are other variants in the swish family:
  - Mish, Hard-swish, Hard-Mish
 
# 16. Max out
Maxout activation function will look at it multiple inputs and it will select the maximum value and return the value.
- Maxout is a generalization of the ReLU and the Leaky ReLU fucntion
- Maxout neuron would have all the benefits of a ReLU neuron without having to be a ReLu
- Maxout is that it is expensive as it doubles the number of parameters for each neuton


#  17. Softmax
Softmax activation function will calculate the probabilities of each class over all possible classes.
When used for multi-classification model it returns the probabilities of each class and the target class will have the high probability.

The calculated probabilities will be in the range of 0 to 1. The sum of all the probabilities is equals to 1.

Softmax function is generally used in multiple classification on the output layer.

# 18. What is an algorithm 
A set of mathematical or computer instructions to perform a specific task. 

# 19. What is an ML model
In general, a model is an informative representation of an object, person ot system.
Models can be:
- concrete - have a physical form ( design for car )
- Abstract - expressed as behavioural patterns (eg mathematical, computer code, words)

# 20. What is ML model?
- An ML model is function that takes in data, perform an ML algorithm to produce a prediction.

# 21. What is a feature
- A feature is a characteristic extracted from out unstructured dataset has been prepared to be ingested by our ML model to infer a prediction
- ML model generally only accepts numerial data, and so we prepare our data into a machine-readable format by **encoding**
What is Feature Engineering?
- Feature engineering is the process of extracting feature from our provided data sources.

# 22. What is inference
INferance is the act of requesting and getting a prediction 
inference relating to ML is when you:
- input data
- into ML model that has been deployed for production use to
- output a prediction

# 23 What is parameter and hyper parameter
- A variable that configures the internal state of a model and whos value can be estimated.
- Thae value of the parameter is now manually set and will be learned outputted after training.
- Parameter are use to make prediction

# 24 What is hyper parameter
- A variable that is external to the model and who's value cannot be estimated
- the value of the hyperparameter is manually set before the training of the model
- Hyperparameters to estimate models parameter
  - learning _rate
  - epochs
  - batch_size
  
# 25 responsible AI
Refer the resource: https://aws.amazon.com/ai/responsible-ai/







