# 1. Bert Lab
Use hugging face website to get code to test a BERT model on google colllab

# 2. BERT with Lola
BERT is developed in 2018 by google
BERT is model with Encoder only model where model input is Natual Language (NL) and output is encoding(Mathematical representation of NL)
Whereas GPT is decoder only model where input is Encoding and output is NL
Bert is 342 million parameters model also has many variants starting from 100M parameters
GPT is 1.2 trillion 
emergent taks model: are model which were better at unexpected scenarios

BERT is pre-trained on following tasks:
- Masked language model (MLM)
  - Provided input where tokens are masked
  - Think of asking BERT to fill in the blanks for sentenses
- Next sentence prediction (NSP)
  - BERT is provided two sentences A anf B
  - BERT has to predict if B would follow A
- BERT can be fine-tuned to then perform the following tasks
  - Named Entity Recognition (NER)
  - Question Answering (QA): Short answers
  - Sentence Pair Tasks
  - Summerization
  - Feature Extraction / Embeddings
  - Classification
  - and more...
- BERT variants
  - roBERTa
  - DistilBERT
  - ALBERT
  - & so on
 
 
- Chinchilla papar: bigger the model need more data and it works best. it gives relation between model size parameter and performance. paramaeters to data is 1:20 ration.
- Distallation: take essence out 


# 4. Sentence Transformaers
Sentence transformer (aka SBERT) is built on top of BERT. It create a signle vector for an entire sentence. When comparing similar senrence this much more
performance than simply using BERT which looks at every single word.
```
# Example model on hugging face
from sentence_transformers import SentenceTransformer, util
model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
```

# 5. Sentence Transformer Lab
www.sbert.net


```
#Embedding model example
from sentence_transformers import SentenceTransformer

# 1. Load a pretrained Sentence Transformer model
model = SentenceTransformer("all-MiniLM-L6-v2")

# The sentences to encode
sentences = [
    "The weather is lovely today.",
    "It's so sunny outside!",
    "He drove to the stadium.",
]

# 2. Calculate embeddings by calling model.encode()
embeddings = model.encode(sentences)
print(embeddings.shape)
# [3, 384]

# 3. Calculate the embedding similarities
similarities = model.similarity(embeddings, embeddings)
print(similarities)
# tensor([[1.0000, 0.6660, 0.1046],
#         [0.6660, 1.0000, 0.1411],
#         [0.1046, 0.1411, 1.0000]])

```

# 6. Intro to Perceptron
Perceptron is an algorithm for supervised learning of binary classifiers invented in 1943 and the machine built in 1957!
A perceptron is a single-neuron model that was a precursor to larger neural networks.









