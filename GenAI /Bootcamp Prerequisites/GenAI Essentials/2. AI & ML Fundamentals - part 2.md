# 1. Bert Lab
Use hugging face website to get code to test a BERT model on google colllab

# 2. BERT with Lola
BERT is developed in 2018 by google
BERT is model with Encoder only model where model input is Natual Language (NL) and output is encoding(Mathematical representation of NL)
Whereas GPT is decoder only model where input is Encoding and output is NL
Bert is 342 million parameters model also has many variants starting from 100M parameters
GPT is 1.2 trillion 
emergent taks model: are model which were better at unexpected scenarios

BERT is pre-trained on following tasks:
- Masked language model (MLM)
  - Provided input where tokens are masked
  - Think of asking BERT to fill in the blanks for sentenses
- Next sentence prediction (NSP)
  - BERT is provided two sentences A anf B
  - BERT has to predict if B would follow A
- BERT can be fine-tuned to then perform the following tasks
  - Named Entity Recognition (NER)
  - Question Answering (QA): Short answers
  - Sentence Pair Tasks
  - Summerization
  - Feature Extraction / Embeddings
  - Classification
  - and more...
- BERT variants
  - roBERTa
  - DistilBERT
  - ALBERT
  - & so on 
- Chinchilla papar: bigger the model need more data and it works best. it gives relation between model size parameter and performance. paramaeters to data is 1:20 ration.
- Distallation: take essence out 




