# 1. AI vs GEN AI
- AI's goal is to interpret, analyse adn response to human action. To simulate humal intelligence in machine
- Gen AI is focuses on creating new content or data

# 2. What is foundational model
A FM is a general purpose model that is trained on vast amounts of data. 

# 3. What is LLM 
- A LLLM is a foundational model that implements the transformaer architecture.
- During training phase, the model learns sematics (patterns) of language, such as grammer, word usage, sentence structure, style and tone.

# 3 Transformaer architecture 
Transformaer architecture was developed by research at Google that is effective at NLP due to **multi-head attention** and **positional encoding**.

- transformer atchiture consist two parts
  - Encoder: Reads and understands the input text
  - Decoder: Based on encoder learning m this part **generates new piece of text**

# 4 Tokenization
It hte process of breaking data input (text) into smaller parts
Tokenization Algorithms:
- Byte Pair Encoding (BPE) used by GPT 3
- WordPiece used by BERT
- SetencePiece used by Google T5 or GPT 3.5
When working with a LLMs, the input text must be converted ( tokenized) into  sequence of token **that match the model's internal vocabulary.**

# 5 Tiken and apacity 
When using transformer the decoder continuosly feeds the sequence of tokens back in as output to help predict the next word in the input.
Capacity requires:
- MemoryL
  - each token in a sequence requiers memory
  - as the token count increases, the memory increses
  - The memory usage eventually becomes exhausted.
- Compute
  - Model performs more operations for each additional token
  - Longer sequences requier more compute

# 6 Embeddings
A vector is an arrow with a length and direction. all vectos are stored in a vector space model. En=mbeddings are vectors data used by ML model to find relationship between data.

ML Model can also create embeddings. different embedding algo capture different kinds of relationships

Embedding can be share across models, you can say its a memory of model.
# 7 Positional encoding 
It is a technique used to **preserve order of words** when processing NL. Transformer need positional encoders because **they do not process data sequencially** and would lose order of understanding when analyze large bodies of text.

# 8 Attention
Attention figures out how each word (token) **in a sequence** is important to other words within that sequence by assigning the words weights.
- Self - attention
  - Computes attention weigths within the same input sequences, where each element attends to all elements
  - Used in transformers to model relationship in sequence (word in sequence)
- Cross-attention
  - Computes attention weigths between two different sequences, allowing one sequence to attend to another sequence
  - Used in task like translation where the output on sequence (decoder) needs to focus on the input sequence (encoder)
- Multi-Head attention
  - Combines self-attention ( or cross attention) headsin parallel, each focusing on different aspects of the input.
  - Used in transformers to improve performance and capture various dependencies simultaneously.

# 9. Fine tunning LLMs
what is fine tunning 
- retraining a pretrained model weight's (parameters) on a smaller dataset.
- A model's weights is the ouputted state of a trained model.

What is supervised fine tunning (SFT)
- When the dataset that been labeled, so we are explicitly telling the model what the data is.
- 









