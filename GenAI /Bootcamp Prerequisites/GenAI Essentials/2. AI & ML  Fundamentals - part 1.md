# AI and ML Fundamentals
## 1. What is AI
- Artificial Intelligence (AI): Machines that perform tasks that **mimic human behavior**.
- Machine Learning (ML): Machines that improve at a task **without explicit programming**.
- Deep Learning (DL): A subset of ML that uses **artificial neural networks**, inspired by the **human brain**, to solve complex problems.
- Generative AI (GenAI): A specialized subset of AI that **generates content** such as:
Image, Video, Text, Audio.

## 2. AI vs Generative AI
### What is Artificial Intelligence (AI)?
AI refers to computer systems that perform tasks typically requiring human intelligence, such as:
- Problem-solving
- Decision-making
- Understanding natural language
- Recognizing speech and images

### AI‚Äôs goal:
To interpret, analyze, and respond to human actions, simulating human intelligence in machines.

- Key Concepts:
  - Simulate: Mimics aspects and behavior.
  - Emulate: Replicates exact processes and mechanisms.

### AI Applications:
- Expert systems
- Natural language processing (NLP)
- Speech recognition
- Robotics

### Industry Use Cases:
- Customer service: B2C chatbots
- E-commerce: Recommendation systems
- Automotive: Autonomous vehicles
- Medical: Diagnosis and treatment support

### What is Generative AI (GenAI)?
A subset of AI focused on creating new content or data that is novel and realistic.
Unlike traditional AI, GenAI can generate new data itself.


Examples:
Generating text, images, music, speech, and other forms of media.

### Key Technologies:
- Generative Adversarial Networks (GANs)
- Variational Autoencoders (VAEs)
- Transformer models (e.g., GPT)

GenAI Modalities(sensens like human touch, listen & so on):
- Vision: Creating realistic images/videos
- Text: Generating human-like text
- Audio: Composing music
- Molecular: Drug discovery using genomic data

### Important Term:

Large Language Models (LLMs) ‚Üí A subset of GenAI that specializes in generating human-like text.
Often confused with general AI due to its popularity

### Comparison: AI vs Generative AI

| Category | Artificial Intelligence (AI) | Generative AI (GenAI)|
|-----:|---------------|---------------|
|     Functionality|Focuses on understanding and decision-making |Focuses on creating new, original content|
|     Data Handling|Uses existing data for analysis and decision-making	|Uses existing data to generate new unseen outputs|
|     Applications| Used across multiple industries for automation, NLP, data analysis, and healthcare|Focuses on creative content generation, deepfakes, and synthetic data|


## 3. What is Jupyter Notebook?
Jupyter Notebook is a web-based application for creating documents that combine:
- Live code
- Narrative text
- Equations
- Visualizations

Originally developed as part of IPython, Jupyter Notebooks evolved into a standalone tool.

### What is JupyterLab?
JupyterLab is the next-generation web-based user interface for Jupyter.
It includes:
- Notebook
- Terminal
- Text Editor
- File Browser
- Rich Outputs

‚úÖ Combines flexibility and power in one interface.
üîÑ JupyterLab will eventually replace Jupyter Notebook.

### What is JupyterHub?
JupyterHub is a server-based platform that runs JupyterLab for multiple users.
Ideal for:
- üë©‚Äçüè´ A class of students
- üè¢ Corporate data science teams
- üî¨ Scientific research groups

It manages authentication, spawns individual notebook instances, and controls access.

## 4.Natural Language Processing (NLP)
- Definition:
Natural Language Processing (NLP) is a Machine Learning technique that enables computers to understand the context of a corpus (a body of related text).
- Key Functions of NLP
  - Analyze and interpret text (e.g., emails, documents).
  - Interpret spoken language (e.g., sentiment analysis).
  - Synthesize speech (e.g., voice assistants).
  - Translate spoken or written phrases between languages.
  - Interpret commands and determine appropriate actions.
### Core Components of NLP
- Text Wrangling and Pre-Processing
  - Conversion ‚Äì Transform text into a standardized format.
  - Sanitization ‚Äì Remove noise, unnecessary characters.
  - Tokenization ‚Äì Split text into words or phrases.
  - Stemming ‚Äì Reduce words to their root form (e.g., "running" ‚Üí "run").
  - Lemmatization ‚Äì Normalize words based on their dictionary meaning.
- Language Understanding (Structure/Syntax)
  - Parts of Speech (POS) Tagging ‚Äì Identify nouns, verbs, adjectives, etc.
  - Chunking ‚Äì Group related words together.
  - Dependency Parsing ‚Äì Analyze grammatical relationships in a sentence.
  - Constituency Parsing ‚Äì Break down a sentence into hierarchical sub-units.
- Processing and Functionality
  - Named Entity Recognition (NER) ‚Äì Identify proper names (e.g., people, places).
  - N-gram Identification ‚Äì Analyze word sequences to predict text.
  - Sentiment Analysis ‚Äì Detect emotions and opinions in text.
  - Information Extraction ‚Äì Identify key information from unstructured data.
  - Information Retrieval ‚Äì Find relevant documents or data.
  - Questions and Answering ‚Äì Process user queries for precise answers.
  - Topic Modeling ‚Äì Identify key themes in text data.

## 5. Regression Study Notes
- What is Regression?
Regression is the process of finding a function to correlate a labeled dataset into a continuous variable/number.
Outcome: Predict a variable in the future, e.g., "What will the temperature be next week?" (20¬∞C).
- Key Concepts
  - Vectors (dots): Represent data points plotted on a graph (e.g., X, Y dimensions).
  - Regression Line: A line drawn through the dataset to minimize the error between predicted and actual values.
  - Error: The distance of the vector (data point) from the regression line.
- Types of Errors Used in Regression
  - Mean Squared Error (MSE):
    - Calculates the average of the squared differences between predicted and actual values.
    - Purpose: Penalizes larger errors more heavily.
    - Use Case: Suitable when large errors are highly critical.
  - Root Mean Squared Error (RMSE)
    - Square root of MSE, returning error to the same units as the target variable (e.g., ¬∞C).
    - Purpose: Easier to interpret due to matching units.
    - Use Case: Commonly used for evaluating model performance.
  - Mean Absolute Error (MAE)
    - Calculates the average of the absolute differences between predicted and actual values.
    - Purpose: Treats all errors equally without amplifying larger deviations.
    - Use Case: Robust to outliers and provides a simple measure of deviation.
- Key Points to Remember
  - Lower Error Values = Better Model Performance.
  - Use MSE or RMSE when larger errors are more problematic.
  - Use MAE for a simpler, more balanced error measurement.
- Example Use Case
  - Predicting the temperature next week based on past weather data using regression analysis.

## 6.Classification
- Definition
Classification is a process of finding a function to divide a **labeled** dataset into classes/categories.
- Example
Predicting a category based on input data.
- Example Question: Will it rain next Saturday?
Possible categories: Sunny, Rainy.
- Key Concept
  - A classification line separates the dataset into different classes.
  - Data on one side belongs to Sunny.
  - Data on the other side belongs to Rainy.
- Classification Algorithms
  - Logistic Regression
  - Decision Tree / Random Forest
  - Neural Networks
  - Na√Øve Bayes
  - K-Nearest Neighbors (KNN)
  - Support Vector Machines (SVM)

## 7. Clustering
Is a algorithum to classify **unlabbled** data 

## 8. Types of Machine Learning
### Learning Problems
- 1. Supervised Learning
  - Definition: The model learns from labeled data to predict outcomes for new, unseen data.
  - Example: Predicting house prices using historical data with features like square footage, number of bedrooms, and location.
  - Input: Data with labels (e.g., features of a house and its price).
  - Output: Predicted price.
  - Key Point: Requires a fully labeled dataset.
- 2. Unsupervised Learning
  - Definition: The model discovers patterns or groupings in data without labels.
  - Example: Customer segmentation in marketing.
  - Input: Customer purchasing behavior without predefined labels.
  - Output: Clusters like "frequent buyers" and "one-time shoppers."
  - Key Point: Focuses on pattern recognition, not predictions.
- 3. Reinforcement Learning
  - Definition: An agent learns by interacting with the environment and receiving feedback in the form of rewards or penalties.
  - Example: A robot learning to walk by adjusting its steps and being rewarded for stability.
  - Input: Current state (e.g., balance).
  - Output: Action (e.g., adjust leg movement).
  - Key Point: Focuses on trial and error.
### Hybrid Learning Problems
- 1. Semi-Supervised Learning
  - Definition: Combines a small amount of labeled data with a large amount of unlabeled data to improve learning.
  - Example: Identifying fraudulent transactions in banking when only a few transactions are labeled as fraudulent.
  - Labeled data: A small subset of transactions.
  - Unlabeled data: Thousands of other transactions.
- 2. Self-Supervised Learning
  - Definition: Automatically generates pseudo-labels from unlabeled data to train a model.
  - Example: Training a language model like GPT using massive amounts of text by predicting the next word in a sentence.
  - Input: "Machine learning is‚Ä¶"
  - Output: The model predicts "fun."
- 3. Multi-Instance Learning
  - Definition: Instead of labeling individual examples, groups (bags) of examples are labeled.
  - Example: Determining whether a medical scan (bag) contains cancerous cells (positive label), even though individual images (instances) are not labeled.
### Statistical Inference
- 1. Inductive Inference
  - Definition: Derives conclusions by observing patterns in the data.
  - Example: After observing that "all apples on the tree are red," infers that "all apples on the tree must be red."
- 2. Deductive Inference
  - Definition: Applies general rules to draw specific conclusions.
  - Example:
Rule: "All cats are mammals."
Observation: "Luna is a cat."
Conclusion: "Luna is a mammal."
- 3. Transductive Inference
  - Definition: Focuses on making specific predictions for known data without generalizing.
  - Example: Predicting student performance on specific test questions without building a general model for all tests.
### Learning Techniques
- 1. Multi-Task Learning
  - Definition: Solves multiple related problems simultaneously.
  - Example: A model trained to recognize both facial expressions (emotion detection) and gender from the same images.
- 2. Active Learning
  - Definition: The model queries a human for labels when it is unsure.
  - Example: A document classification system asks a human to label ambiguous emails (e.g., "spam or not spam?").
- 3. Online Learning
  - Definition: Continuously updates the model with new data.
  - Example: Stock market prediction models that are updated as new trading data arrives.
- 4. Transfer Learning
  - Definition: A pre-trained model is adapted for a new, related task.
  - Example:
    - Task 1: Training a model on a large dataset of general images.
    - Task 2: Using the trained model to recognize medical images like X-rays.
- 5. Ensemble Learning
  - Definition: Combines multiple models to improve performance.
  - Example:
    - Random Forest: Combines multiple decision trees.
    - Voting Ensemble: Uses the majority vote from multiple classifiers for prediction.
### Key Takeaways
- Supervised Learning: Requires labeled data.
- Unsupervised Learning: Focuses on finding patterns in unlabeled data.
- Reinforcement Learning: Uses feedback (reward/punishment) to learn actions.
- Hybrid Problems: Combine supervised and unsupervised methods for specialized tasks.
- Techniques: Focus on optimization (e.g., transfer learning for efficiency, ensemble for accuracy).

## 8. Divisions of Machine Learning
For Machine Learning, we can divide it into the following categories:
### Classical Machine Learning
- Usage: For structured data with clear features.
- Supervised Learning (e.g., regression, classification)
- Unsupervised Learning (e.g., clustering, dimensionality reduction)
### Reinforcement Learning
- Usage: When models need to learn optimal actions in an environment with no direct supervision.
- Applications: Real-time decision-making, Game AI, Robotics, and Self-driving cars.
### Ensemble Methods
- Usage: When the quality of predictions is improved by combining multiple models.
- Techniques:
  - Bagging (e.g., Random Forest)
  - Boosting (e.g., XGBoost)
  - Stacking
### Neural Networks and Deep Learning**
- Usage: For unstructured data (images, text, audio) or when features are complex and unclear.
- Techniques:
  - Convolutional Neural Networks (CNNs): Used for image and video processing.
  - Recurrent Neural Networks (RNNs): Used for time-series or sequential data.
  - Generative Adversarial Networks (GANs): Used for data generation tasks.
  - Multilayer Perceptron (MLP): Used for simple feedforward neural network tasks.
  - Autoencoders: Used for dimensionality reduction or anomaly detection.
## 9. Classical ML
- Supervised: unisng labled data
  - Classification: identify fraud detection
  - Regression: Market forecast
- Unsupervised
  - Clustering:  Target marketing
  - Association: Customer recommendation
  - Dimensionality Reduction: Help to reducing data we process
## 10. Supervised vs Unsupervised
SL is more accurate than UL and UL requires human intervention to validate the results
## 11. SL vs UL vs Reinforcement Learning (RL)
- RL : not data reqiuired, there is environment and ML model generates data and many attempts to reach goal.- GAME AI. Robot navigation
## 12. Supervised Learning Models
- What is Classification?
Classification is the process of finding a function to divide a dataset into classes/categories.
Example: Predicting whether tomorrow's temperature will be hot or cold.
- Common Classification Algorithms
  - Logistic Regression
  - K-Nearest Neighbors
  - Support Vector Machines (SVM)
  - Kernel SVM
  - Na√Øve Bayes
  - Decision Tree Classification
  - Random Forest Classification
- What is Regression?
Regression is the process of finding a function to correlate a dataset into a continuous variable/number.
Example: Predicting tomorrow‚Äôs temperature in degrees.
- Common Regression Algorithms
  - Simple Linear Regression
  - Multiple Linear Regression
  - Polynomial Regression
  - Support Vector Regression (SVR)
  - Decision Tree Regression
  - Random Forest Regression
## 13.  Unsupervised Models
  - Clustering:  Target marketing
    - KMean
    - DB Scan
    - K-Mode
  - Association: Customer recommendation- Product with relation 
    - Apriori
    - Euclat
    - FP-Growth
  - Dimensionality Reduction: Help to reducing data we process
    - Principal Component Analysis (PCA)
    - Linear Discriminant Analysis (LDA)
    - Generalized Discreminant Analysis (GDA)
    - Singular Value Decomposition (SVD)
    - Latent Dirichlet Allocation (LDA)
    - Latent Symantic Analysis (LSA)
    - t-SNE

## 14. Neural Networks and Deep Learning
### What are Neural Networks (NN)?
Often described as mimicking the brain
A neuron/node represents an algorithm
Data flows from one node to another based on weighted connections

- Layers:
Input Layer ‚Üí Hidden Layers ‚Üí Output Layer
- Key Concepts
  - Activation Function: Algorithm applied to a node's output (e.g., ReLU)
  - Dense Layer: Increases number of nodes in the next layer
  - Sparse Layer: Decreases number of nodes in the next layer
  - Loss Function: Measures error by comparing prediction to the ground truth
### What is Deep Learning?
A neural network with 3 or more hidden layers
- Types of Neural Networks
  - Supervised DL Algorithms
    - Fully-connected Feed Forward (FNN)
    - Recurrent Neural Networks (RNN)
    - Convolutional Neural Networks (CNN)
  - Unsupervised DL Algorithms
    - Deep Belief Networks (DBN)
    - Stacked Autoencoders (SAE)
    - Restricted Boltzmann Machines (RBMs)
### Other Important Terms
- Feed Forward Neural Network (FNN): No cycles, data moves in one direction
- Backpropagation (BP): Adjusts weights by moving backward from the output layer to minimize error (key for learning)

## 15. BERT (Bidirectional Encoder Representations from Transformers)
### Overview
- Developed by Google in 2018.
- BERT is based on the Transformer architecture but only uses the encoder part.
- You can access BERT models at: https://huggingface.co/google-bert
- Bi-directional: BERT reads text from left to right and right to left to better understand the context.

### Pretraining Tasks
BERT is pre-trained on two key tasks:
- Masked Language Modeling (MLM):
  - Random words are masked in a sentence.
  - BERT is trained to predict the missing words.
  - Example: "I love to [MASK] pizza" ‚Üí "eat"
- Next Sentence Prediction (NSP):
  - BERT receives two sentences and must predict if the second sentence logically follows the first.

### Fine-tuning Tasks
After pretraining, BERT can be fine-tuned for downstream NLP tasks such as:

- Named Entity Recognition (NER)
- Question Answering (QA)
- Sentence pair classification
- Summarization
- Feature extraction / Embeddings
### Model Sizes
- BERT Base: 110M parameters
- BERT Large: 340M parameters
- BERT Tiny: 4M parameters
- And ~24 other variations

### Popular BERT Variants
- roBERTa
- DistilBERT
- ALBERT
- ELECTRA
- DeBERTa

### Example Use Case: Sentiment Analysis
Using the Hugging Face Transformers library:
```
from transformers import pipeline

classifier = pipeline("sentiment-analysis", 
                      model="bert-base-uncased", 
                      tokenizer="bert-base-uncased")

sentences = [
    "I love using BERT for natural language processing tasks!",
    "I am not a fan of waiting in long lines."
]

results = classifier(sentences)

for sentence, result in zip(sentences, results):
    print(f"Sentence: {sentence}")
    print(f"Prediction: {result['label']} | Score: {result['score']:.4f}")
```

In this example, Hugging Face loads a pretrained and fine-tuned BERT model for sentiment analysis.
### Final Note
BERT remains a strong baseline in NLP, despite newer models being released. It is widely used in research and production.
